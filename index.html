<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9M1MPTBJV2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-9M1MPTBJV2');
  </script>
  <meta charset="utf-8">
  <meta name="description"
        content="The Ingredients for Robotic Diffusion Transformers">
  <meta name="keywords" content="Imitation Learning, Diffusion Policy">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DiT-Policy</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column ">
          <h1 class="title is-1 publication-title" style="font-weight:50%">
            DiT-Block Policy
            <br>
            <span style="font-size:2.1rem; font-weight:6">The Ingredients for Robotic Diffusion Transformers</span>
          </h1>
          
          <div class="is-size-5 publication-authors">

            <span class="author-block">
              <a style="font-weight: 500;" href="https://sudeepdasari.github.io/" target="_blank">Sudeep Dasari</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a style="font-weight: 500;" href="https://www.oiermees.com/" target="_blank">Oier Mees</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a style="font-weight: 500;" href="http://linkedin.com/in/sebbyzhao/" target="_blank">Sebastian Zhao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a style="font-weight: 500;" href="https://scholar.google.com/citations?user=Yu18Q6MAAAAJ&hl=en/" target="_blank">Mohan Kumar Srirama</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a style="font-weight: 500;" href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank">Sergey Levine</a><sup>2</sup>
            </span>
          <!-- <div class="column"> -->
            <br>
            <span class="author-block"><sup><font size="-0.4"><font size="-0.4">1 </sup>Carnegie Mellon University</font>, <sup><font size="-0.4">2 </sup>UC Berkeley</font></span>
             
            <!-- <br><br> --> 
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block" >
                <a href="./resources/paper.pdf"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark" >
                  <span class="icon" style="color:#f5f3f3">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span style="color:#f5f3f3">Paper</span>
                </a>
              </span> 
              <span class="link-block">
                <a href="https://github.com/SudeepDasari/data4robotics/tree/dit_release"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span style="color:#f5f3f3" class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span style="color:#f5f3f3">Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://console.cloud.google.com/storage/browser/aloha_play_dataset_public;tab=objects?forceOnBucketsSortingFiltering=true&authuser=2&project=rail-tpus&prefix=&forceOnObjectsSortingFiltering=false"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span style="color:#f5f3f3" class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span style="color:#f5f3f3">Dataset</span>
                </a>
              </span>
            </div>
          <!-- </div> -->
        </div>
      </div>
    </div>
  </div>
</section>



<!-- <section class="hero teaser"> -->
    <div class="container is-max-desktop">
        <!-- <div class="hero-body"> -->
            <video preload="auto" class="" id="home_vid" autoplay controls muted loop playsinline height="100%" width="100%" controls loop>
                <source src="./static/videos/dit-policy/teaser.mp4"
                        type="video/mp4">
            </video>
            <br><br>
            <h2 class="subtitle has-text-centered scroll-element" >
                Our method allows for stable training of high-capacity Diffusion Transformer Policies.
            </h2>
        <!-- </div> -->
    </div>
<!-- </section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered"> -->
      <!-- <div class="column is-four-fifths"> -->
        <br>
        <h2 class="title is-3 scroll-element">Overview</h2>
        <div class="content has-text-justified scroll-element">
          <p>
            This paper identifies, studies, and improves key architectural design decisions for high-capacity diffusion transformer policies. Our model, named DiT-Block Policy, makes a few simple, yet impactful, changes to the <a href="https://diffusion-policy.cs.columbia.edu">vanilla diffusion transformer recipe</a>. Namely, we add <a href="https://www.wpeebles.com/DiT">adaLN-zero layers</a> to the transformer blocks and further optimize the input encoding layers. The resulting architecture significantly outperforms the state of the art in solving long-horizon (1500+ time-steps) dexterous tasks on a bi-manual ALOHA robot, without the excruciating pain of per-setup hyper-parameter tuning. Finally, we find that our policies show improved scaling performance when trained on 10 hours of highly multi-modal, language annotated ALOHA demonstration data, which we've open-sourced for the community's benefit.
          </p>
          
        </div>
      <!-- </div> -->
    <!-- </div> -->
  </div>
</section>
<!--/ Abstract. -->

<section class="section">  
  <div class="container is-max-desktop">
  
  <div class="container is-max-desktop">
    <!-- <div class="columns is-centered has-text-centered"> -->
      <!-- <div class="column is-four-fifths"> -->
        <h2 class="title is-3 scroll-element">Approach</h2>
        <br>
        <!-- <div class="hero-body"> -->
          <img id="teaser" src="./static/images/aloha_diffusion_arch.png" alt="CrossFormer teaser image."/>
          <br>
          <p style="text-align:center"> DiT-Block Policy Architecture.</p>
        <br>
        <!-- </div> -->
        <div class="content has-text-justified scroll-element">
          <p>
            Our method -- DiT-Block Policy -- is a Transformer neural network architecture designed specifically to be a highly performant conditional noise network for robotic diffusion policies. 
            The DiT-Block Policy architecture is visualized above. First, the text goal and robot proprioception inputs are encoded into observation vectors. 
            Similarly, the time-step (k) is turned into an embedding vector using sinusoidal Fourier Features and a small MLP network. 
            Then, all these embedding vectors are combined with the input noise vector using an encoder-decoder Transformer architecture to produce the denoising output.
            We make <i>two simple, yet critical, additions</i> that significantly boost performance: <b>(1)</b> we replace standard cross-attention layers with <a href="https://www.wpeebles.com/DiT">adaLN-zero layers</a> in the transformer decoder; 
            and <b>(2)</b> we parameterize the image encoders with separate ResNet-18 networks per camera input. Both of these changes significantly boost training stability and final performance.
          </p>

        </div>
      <!-- </div> -->
    <!-- </div> -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

  <div class="container is-max-desktop">
    <!-- <div class="columns is-centered has-text-centered"> -->
      <!-- <div class="column is-four-fifths"> -->
        <h2 class="title is-3 scroll-element">BiPlay Dataset</h2>
        <br>
        <!-- <div class="hero-body"> -->
          <div style="text-align: center;">
          <img id="teaser" src="./static/images/aloha_dataset.png" alt="CrossFormer teaser image." style="width: 65%;"/>
          </div>
          <br>
          <p style="text-align:center"> BiPlay Dataset</p>
        <br>
        <!-- </div> -->
        <div class="content has-text-justified scroll-element">
          <p>
            Inspired by prior work on data scaling<a href="https://robotics-transformer-x.github.io">[1</a>,<a href="https://rail-berkeley.github.io/bridgedata/">2</a>,<a href="https://droid-dataset.github.io">3]</a>, 
            we seek to understand how Dit-Block Policies will behave as they are trained on increasingly diverse demonstrations data. 
            However, the few (open-source) bi-manual datasets that do exist<a href="https://tonyzhaozh.github.io/aloha/">[4</a>,<a href="https://yay-robot.github.io">5]</a>, only consist of a handful of tasks, collected using the same controlled scenes/objects. 
            As a result, they are not useful for testing generalization in our bi-manual setting. To address this shortcoming, we collected and annotated BiPlay, a more diverse bi-manual manipulation dataset with randomized objects and background settings. 
            We collected BiPlay as a series of 3.5 minute long episodes. For each episode, we constructed a random scene with various objects, and solved a sequence of tasks within that scene. 
            After collection, the episodes were broken into clips that were in turn annotated with appropriate language task descriptions. 
            The final dataset contains 7023 clips spanning 10 Hrs of robot data collection.

        </div>
      <!-- </div> -->
    <!-- </div> -->
  </div>
</section>
<!--/ Abstract. -->

<!-- Architecture Diagram -->
<!-- <section class="hero teaser scroll-element">
    
    
</section> -->
<!--/ Architecture Diagram. -->

<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- <div class="columns is-centered has-text-centered"> -->
        <h2 class="title is-3 scroll-element">Results</h2>
        <!-- <div class="hero-body"> -->
          <br>
          <img class="scroll-element" id="teaser" src="./resources/chart.jpg" alt="CrossFormer Results"/>
        <!-- </div> -->
         <br>
        
        </div>

      </div>
    </div> 
    <!--/ Animation. -->

  </div>
</section>

<!-- <section class="section scroll-element" id="BibTeX"> -->
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> @inproceedings{dasari2024ditpi,
        title={The Ingredients for Robotic Diffusion Transformers},
        author = {Mohan Kumar Srirama and Sudeep Dasari and Shikhar Bahl and Abhinav Gupta},
        booktitle = {arXiv e-prints},
        year={2024},
    }</code></pre>
  </div>

<!-- </section> -->

<!-- <footer class="footer"> -->
  <div class="container" style="padding-bottom:40px">
    <div class="content has-text-centered">
      <br>
      <br>
      <p>
        This website was adapted from the following <a
          href="https://github.com/nerfies/nerfies.github.io" target="_blank">source code</a>.
      </p>
    </div>
  </div>
<!-- </footer> -->

<script>
  var home_vid = document.getElementById('home_vid');
  home_vid.addEventListener('canplay', function() {
    home_vid.play();
  });
  // Access the video element
  var nav_video = document.getElementById('nav_vid');
  nav_video.addEventListener('loadedmetadata', function() {
    nav_video.playbackRate = 2.0;
  });
  nav_video.addEventListener('canplay', function() {
    nav_video.play();
  });

  var manip_vid = document.getElementById('manip_vid');
  manip_vid.addEventListener('loadedmetadata', function() {
    manip_vid.playbackRate = 2.0;
  });
  manip_vid.addEventListener('canplay', function() {
    manip_vid.play();
  });

  var locomotion_vid = document.getElementById('locomotion_vid');
  locomotion_vid.addEventListener('loadedmetadata', function() {
    locomotion_vid.playbackRate = 2.0;
  });
  locomotion_vid.addEventListener('canplay', function() {
    locomotion_vid.play();
  });

  var simple_vid = document.getElementById('simple_vid');
  simple_vid.addEventListener('loadedmetadata', function() {
    simple_vid.playbackRate = 2.0;
  });
  simple_vid.addEventListener('canplay', function() {
    simple_vid.play();
  });

  var obs1_vid = document.getElementById('obs1_vid');
  obs1_vid.addEventListener('loadedmetadata', function() {
    obs1_vid.playbackRate = 2.0;
  });
  obs1_vid.addEventListener('canplay', function() {
    obs1_vid.play();
  });

  var obs2_vid = document.getElementById('obs2_vid');
  obs2_vid.addEventListener('loadedmetadata', function() {
    obs2_vid.playbackRate = 2.0;
  });
  obs2_vid.addEventListener('canplay', function() {
    obs2_vid.play();
  });

  var sharpturn_vid = document.getElementById('sharpturn_vid');
  sharpturn_vid.addEventListener('loadedmetadata', function() {
    sharpturn_vid.playbackRate = 2.0;
  });
  sharpturn_vid.addEventListener('canplay', function() {
    sharpturn_vid.play();
  });

</script>

</body>
</html>
